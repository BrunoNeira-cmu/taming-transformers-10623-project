{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a6ad92-39c9-425e-a5bf-448b8ff01f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x70b8fc887910>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "\n",
    "# also disable grad to save memory\n",
    "import torch\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b125ae-8c55-4c19-89b2-71e2c8a949e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from taming.models.vqgan import VQModel, GumbelVQ\n",
    "\n",
    "def load_config(config_path, display=False):\n",
    "  config = OmegaConf.load(config_path)\n",
    "  if display:\n",
    "    print(yaml.dump(OmegaConf.to_container(config)))\n",
    "  return config\n",
    "\n",
    "def load_vqgan(config, ckpt_path=None, is_gumbel=False):\n",
    "  if is_gumbel:\n",
    "    model = GumbelVQ(**config.model.params)\n",
    "  else:\n",
    "    model = VQModel(**config.model.params)\n",
    "  if ckpt_path is not None:\n",
    "    sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "    missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "  return model.eval()\n",
    "\n",
    "def preprocess_vqgan(x):\n",
    "  x = 2.*x - 1.\n",
    "  return x\n",
    "\n",
    "def custom_to_pil(x):\n",
    "  x = x.detach().cpu()\n",
    "  x = torch.clamp(x, -1., 1.)\n",
    "  x = (x + 1.)/2.\n",
    "  x = x.permute(1,2,0).numpy()\n",
    "  x = (255*x).astype(np.uint8)\n",
    "  x = Image.fromarray(x)\n",
    "  if not x.mode == \"RGB\":\n",
    "    x = x.convert(\"RGB\")\n",
    "  return x\n",
    "\n",
    "def reconstruct_with_vqgan(x, model):\n",
    "  # could also use model(x) for reconstruction but use explicit encoding and decoding here\n",
    "  print(f\"Input to vqgan shape = {x.shape}\")\n",
    "  z, _, [_, _, indices] = model.encode(x)\n",
    "  print(z.shape)\n",
    "  print(f\"VQGAN --- {model.__class__.__name__}: latent shape: {z.shape[2:]}\")\n",
    "  xrec = model.decode(z)\n",
    "  return xrec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4c4083d-e5d7-40cc-9b86-216f14bfb68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os, sys\n",
    "import requests\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from IPython.display import display, display_markdown\n",
    "\n",
    "\n",
    "def download_image(url):\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return PIL.Image.open(io.BytesIO(resp.content))\n",
    "\n",
    "\n",
    "def preprocess(img, target_image_size=256, map_dalle=True):\n",
    "    s = min(img.size)\n",
    "\n",
    "    if s < target_image_size:\n",
    "        raise ValueError(f'min dim for image {s} < {target_image_size}')\n",
    "\n",
    "    r = target_image_size / s\n",
    "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "    img = TF.resize(img, s, interpolation=PIL.Image.LANCZOS)\n",
    "    img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
    "    img = torch.unsqueeze(T.ToTensor()(img), 0)\n",
    "    return img\n",
    "\n",
    "def stack_reconstructions(input, x0, titles=[]):\n",
    "  print(input.size)\n",
    "  print(x0.size)\n",
    "  assert input.size == x0.size\n",
    "  w, h = input.size[0], input.size[1]\n",
    "  img = Image.new(\"RGB\", (2*w, h))\n",
    "  img.paste(input, (0,0))\n",
    "  img.paste(x0, (1*w,0))\n",
    "  for i, title in enumerate(titles):\n",
    "    ImageDraw.Draw(img).text((i*w, 0), f'{title}', (255, 255, 255)) # coordinates, text, color, font\n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd788d58-1f5f-44e8-81db-1c6fa6980bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Text 2 Image via Multimodal Fusion\n",
    "Authors: Nicholas Mesa-Cucalon, Bruno Neira, Deon D Kouatchou-Ngongang\n",
    "10-623 Generative AI\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import sys\n",
    "import os\n",
    "os.chdir(\"/home/ubuntu/taming-transformers-10623-project\")\n",
    "sys.path.append(os.getcwd())\n",
    "import yaml\n",
    "from taming.models.vqgan import VQModel, GumbelVQ\n",
    "import numpy as np\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from transformers import CLIPModel, AutoProcessor\n",
    "\n",
    "\"\"\"\n",
    "Helper Functions\n",
    "\"\"\"\n",
    "def preprocess_vqgan(x):\n",
    "  x = 2.*x - 1.\n",
    "  return x\n",
    "\n",
    "def load_config(config_path, display=False):\n",
    "  config = OmegaConf.load(config_path)\n",
    "  if display:\n",
    "    print(yaml.dump(OmegaConf.to_container(config)))\n",
    "  return config\n",
    "\n",
    "def load_vqgan(config, ckpt_path=None, is_gumbel=False):\n",
    "  if is_gumbel:\n",
    "    model = GumbelVQ(**config.model.params)\n",
    "  else:\n",
    "    model = VQModel(**config.model.params)\n",
    "  if ckpt_path is not None:\n",
    "    sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "    missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "  return model.eval()\n",
    "\n",
    "\"\"\"\n",
    "Model 2: Fusion Encoder Input\n",
    "\"\"\"\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class T2IFusionEncoderInput(nn.Module):\n",
    "    def __init__(self, d_proj : int,\n",
    "                       d_embd : int,\n",
    "                       num_chans : int,\n",
    "                       config_path : str,\n",
    "                       ckpt_path : str,\n",
    "                       is_gumbel : bool,\n",
    "    ):\n",
    "        super(T2IFusionEncoderInput, self).__init__()\n",
    "        # Store variables\n",
    "        self.d_proj = d_proj\n",
    "        self.d_embd = d_embd\n",
    "        self.num_channels = num_chans\n",
    "        # CLIP Model\n",
    "        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "        self.clip_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        # Initialize Linear Layer\n",
    "        self.grounding_layer = nn.Linear(d_proj, self.num_channels * (self.d_embd ** 2))\n",
    "        # Initialize Fusion Function\n",
    "        self.fuse_fn = lambda x,y : x + y\n",
    "        # Initialize VQGAN Config\n",
    "        config = load_config(\"logs/vqgan_gumbel_f8/configs/model.yaml\", display=False)\n",
    "        self.vqgan = load_vqgan(config, ckpt_path=ckpt_path, is_gumbel=is_gumbel).to(device)\n",
    "        # Freeze CLIP Modules\n",
    "        for param in self.clip.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Freeze VQGAN modules\n",
    "        for param in self.vqgan.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Batch size is the length of the text list\n",
    "        b = len(x)\n",
    "        # Process text and image into features\n",
    "        txt_in = self.clip_processor.tokenizer(\n",
    "            text=x, return_tensors=\"pt\", padding=True, truncation = True,\n",
    "        ).to(device)\n",
    "        img_in = self.clip_processor.image_processor(\n",
    "            images=y, do_rescale = False, return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "        txt_features = self.clip.get_text_features(**txt_in)\n",
    "        img_features = self.clip.get_image_features(**img_in)\n",
    "        # Fuse features\n",
    "        fused_features = self.fuse_fn(txt_features,img_features)\n",
    "        # Project it into the encoder input dimension\n",
    "        embd = self.grounding_layer(fused_features)\n",
    "        # Create a [b, 3, d_embd, d_embd] version of our grounded text features\n",
    "        embd = embd.reshape((b,self.num_channels,self.d_embd,self.d_embd))\n",
    "        # Preprocess embedding for the VQGAN Encoder\n",
    "        embd = preprocess_vqgan(embd)\n",
    "        # Encode embedding with VQGAN Encoder\n",
    "        z, _, _ = self.vqgan.encode(embd)\n",
    "        # Return a reconstructed version of the encoded embedding\n",
    "        return self.vqgan.decode(z)\n",
    "\n",
    "    def encode_text(self, x):\n",
    "        # Tokenize text\n",
    "        inputs = self.clip_processor.tokenizer(x, padding = \"max_length\",\n",
    "            max_length = 77, truncation = True, return_tensors = \"pt\").to(device)\n",
    "        # Get text features\n",
    "        txt_features = self.clip.get_text_features(**inputs)\n",
    "        # Get the batch size\n",
    "        b, _ = txt_features.shape\n",
    "        # Project it into the encoder input dimension\n",
    "        text_embd = self.grounding_layer(txt_features)\n",
    "        # Return a [b, 3, d_embd, d_embd] view of our grounded text features\n",
    "        return text_embd.view((b,self.num_channels,self.d_embd,self.d_embd))\n",
    "\n",
    "    def generate(self,x):\n",
    "        # Encode text\n",
    "        x = self.encode_text(x)\n",
    "        # Preprocess encoded text for the VQGAN\n",
    "        x = preprocess_vqgan(x)\n",
    "        # Encode text with VQGAN Encoder\n",
    "        z, _, _ = self.vqgan.encode(x)\n",
    "        # Return a reconstructed version of the encoded text embedding\n",
    "        return self.vqgan.decode(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74f1bf15-4d75-4631-ae97-ea9c6c5af7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 32, 32) = 262144 dimensions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11985/1817328416.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 32, 32) = 262144 dimensions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11985/2465187177.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input is of size: torch.Size([1, 3, 32, 32])\n",
      "Input to vqgan shape = torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 256, 4, 4])\n",
      "VQGAN --- GumbelVQ: latent shape: torch.Size([4, 4])\n",
      "(32, 32)\n",
      "(32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x32 at 0x70B7FFD26080>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config32x32 = load_config(\"logs/vqgan_gumbel_f8/configs/model.yaml\", display=False)\n",
    "model32x32 = load_vqgan(config32x32, ckpt_path=\"logs/vqgan_gumbel_f8/checkpoints/last.ckpt\", is_gumbel=True).to(device)\n",
    "config_path = \"logs/vqgan_gumbel_f8/configs/model.yaml\"\n",
    "chkpt_path = \"logs/vqgan_gumbel_f8/checkpoints/last.ckpt\"\n",
    "model = T2IFusionEncoderInput(512, 256, 3, config_path, chkpt_path, True)\n",
    "model_path = \"fusion_input.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "titles=[\"Input\", \"VQGAN (f8, 8192)\"]\n",
    "\n",
    "def reconstruction_pipeline(url, size=320):\n",
    "  x_vqgan = preprocess(download_image(url), target_image_size=size, map_dalle=False)\n",
    "  x_vqgan = x_vqgan.to(device)\n",
    "\n",
    "  print(f\"input is of size: {x_vqgan.shape}\")\n",
    "  x0        = reconstruct_with_vqgan(preprocess_vqgan(x_vqgan), model32x32)\n",
    "  x0_t2i    = model.generate([\"cat\"])\n",
    "  pil_input = custom_to_pil(preprocess_vqgan(x_vqgan[0]))\n",
    "  pil_x1    = custom_to_pil(x0_t2i[0])\n",
    "  img       = stack_reconstructions(pil_input, pil_x1, titles=titles)\n",
    "  return img\n",
    "\n",
    "reconstruction_pipeline(url='https://heibox.uni-heidelberg.de/f/6f12b330eb564d288d76/?dl=1', size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd96c464-1be8-4393-8f30-d96f0cb493a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
