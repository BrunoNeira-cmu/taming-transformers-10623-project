{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "#                     IMPORTS                        #\n",
    "######################################################\n",
    "\n",
    "import yaml\n",
    "from omegaconf import OmegaConf\n",
    "from taming.models.vqgan import VQModel, GumbelVQ\n",
    "import io\n",
    "import os, sys\n",
    "import requests\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from IPython.display import display, display_markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################################################################\n",
    "#                              CONSTANTS                                #\n",
    "#########################################################################\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSans-BoldItalic.ttf\", 22)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "titles=[\"Input\", \"DALL-E dVAE (f8, 8192)\", \"VQGAN (f8, 8192)\", \n",
    "        \"VQGAN (f16, 16384)\", \"VQGAN (f16, 1024)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################################################################\n",
    "#                            HELPER FUNCTIONS                           #\n",
    "#########################################################################\n",
    "\n",
    "def load_config(config_path, display=False):\n",
    "  config = OmegaConf.load(config_path)\n",
    "  if display:\n",
    "    print(yaml.dump(OmegaConf.to_container(config)))\n",
    "  return config\n",
    "\n",
    "def load_vqgan(config, ckpt_path=None, is_gumbel=False):\n",
    "  if is_gumbel:\n",
    "    model = GumbelVQ(**config.model.params)\n",
    "  else:\n",
    "    model = VQModel(**config.model.params)\n",
    "  if ckpt_path is not None:\n",
    "    sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "    missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "  return model.eval()\n",
    "\n",
    "def preprocess_vqgan(x):\n",
    "  x = 2.*x - 1.\n",
    "  return x\n",
    "\n",
    "def custom_to_pil(x):\n",
    "  x = x.detach().cpu()\n",
    "  x = torch.clamp(x, -1., 1.)\n",
    "  x = (x + 1.)/2.\n",
    "  x = x.permute(1,2,0).numpy()\n",
    "  x = (255*x).astype(np.uint8)\n",
    "  x = Image.fromarray(x)\n",
    "  if not x.mode == \"RGB\":\n",
    "    x = x.convert(\"RGB\")\n",
    "  return x\n",
    "\n",
    "def reconstruct_with_vqgan(x, model):\n",
    "  # could also use model(x) for reconstruction but use explicit encoding and decoding here\n",
    "  z, _, [_, _, indices] = model.encode(x)\n",
    "  print(f\"VQGAN --- {model.__class__.__name__}: latent shape: {z.shape[2:]}\")\n",
    "  xrec = model.decode(z)\n",
    "  return xrec\n",
    "\n",
    "def download_image(url):\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return PIL.Image.open(io.BytesIO(resp.content))\n",
    "\n",
    "\n",
    "def preprocess(img, target_image_size=256):\n",
    "    s = min(img.size)\n",
    "\n",
    "    if s < target_image_size:\n",
    "        raise ValueError(f'min dim for image {s} < {target_image_size}')\n",
    "\n",
    "    r = target_image_size / s\n",
    "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "    img = TF.resize(img, s, interpolation=PIL.Image.LANCZOS)\n",
    "    img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
    "    img = torch.unsqueeze(T.ToTensor()(img), 0)\n",
    "\n",
    "    return img\n",
    "\n",
    "def stack_reconstructions(input, x0, x1, x2, x3, titles=[]):\n",
    "  assert input.size == x1.size == x2.size == x3.size\n",
    "  w, h = input.size[0], input.size[1]\n",
    "  img = Image.new(\"RGB\", (5*w, h))\n",
    "  img.paste(input, (0,0))\n",
    "  img.paste(x0, (1*w,0))\n",
    "  img.paste(x1, (2*w,0))\n",
    "  img.paste(x2, (3*w,0))\n",
    "  img.paste(x3, (4*w,0))\n",
    "  for i, title in enumerate(titles):\n",
    "    ImageDraw.Draw(img).text((i*w, 0), f'{title}', (255, 255, 255), font=font) # coordinates, text, color, font\n",
    "  return img\n",
    "\n",
    "def reconstruction_pipeline(url, model, size=320):\n",
    "  x_vqgan = preprocess(download_image(url), target_image_size=size)\n",
    "  x_vqgan = x_vqgan.to(DEVICE)\n",
    "\n",
    "  print(f\"input is of size: {x_vqgan.shape}\")\n",
    "  x0 = reconstruct_with_vqgan(preprocess_vqgan(x_vqgan), model)\n",
    "  \n",
    "  img = stack_reconstructions(custom_to_pil(preprocess_vqgan(x_vqgan[0])),\n",
    "                              custom_to_pil(x0[0]), titles=titles)\n",
    "  return img"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
